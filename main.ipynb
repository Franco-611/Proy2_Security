{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2\n",
    "\n",
    "## Diego Franco - 20240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Lee el archivo CSV\n",
    "data = pd.read_csv('BalanceData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Eliminar la columna 'trans_date_trans_time'\n",
    "data.drop(columns=['trans_date_trans_time', 'transaction_time'], inplace=True)\n",
    "\n",
    "# Codificar variables categóricas con label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job']\n",
    "for col in categorical_cols:\n",
    "    data[col + '_encoded'] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Eliminar columnas originales no numéricas y otras columnas irrelevantes\n",
    "data.drop(columns=['merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job', 'dob', 'trans_num'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento, conjunto de validación (dev) y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('is_fraud', axis=1), data['is_fraud'], test_size=0.2, random_state=42)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Imputar valores faltantes con la estrategia de relleno con el valor medio\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_dev_imputed = imputer.transform(X_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo SVM...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer \n",
    "import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    # Predecir probabilidades\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "    precision = precision_score(y, model.predict(X))\n",
    "    recall = recall_score(y, model.predict(X))\n",
    "    f1 = f1_score(y, model.predict(X))\n",
    "    \n",
    "    return roc_auc, precision, recall, f1\n",
    "\n",
    "# Inicializar los modelos\n",
    "ann_model = MLPClassifier()\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "rf_model = RandomForestClassifier()\n",
    "svm_model = SVC(probability=False)  # Habilitar la predicción de probabilidades para SVM\n",
    "\n",
    "# Diccionario de modelos\n",
    "models = {\n",
    "    #\"ANN\": ann_model,\n",
    "    #\"LightGBM\": lgb_model,\n",
    "    #\"XGBoost\": xgb_model,\n",
    "    #\"Random Forest\": rf_model,\n",
    "    \"SVM\": svm_model\n",
    "}\n",
    "\n",
    "# Imputar valores faltantes con la estrategia de relleno con el valor medio\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_dev_imputed = imputer.transform(X_dev)\n",
    "\n",
    "# Diccionario para almacenar métricas antes y después del entrenamiento incremental\n",
    "metrics_before_incremental = {}\n",
    "metrics_after_incremental = {}\n",
    "\n",
    "# Iterar sobre cada modelo\n",
    "for model_name, model in models.items():\n",
    "\n",
    "    print(f\"Entrenando modelo {model_name}...\")\n",
    "    # Entrenamiento inicial\n",
    "    model.fit(X_train_imputed, y_train)  # Utilizar X_train_imputed\n",
    "\n",
    "    # Guardar el modelo antes del entrenamiento incremental\n",
    "    initial_model_filename = f\"{model_name}_initial_model.pkl\"\n",
    "    joblib.dump(model, initial_model_filename)\n",
    "    print(f\"Modelo {model_name} guardado exitosamente.\")\n",
    "    \n",
    "    # Evaluación inicial\n",
    "    roc_auc_before, precision_before, recall_before, f1_before = evaluate_model(model, X_dev_imputed, y_dev)  # Utilizar X_dev_imputed\n",
    "    metrics_before_incremental[model_name] = (roc_auc_before, precision_before, recall_before, f1_before)\n",
    "    \n",
    "    # Entrenamiento incremental con nuevos datos nunca antes utilizados por el modelo\n",
    "    X_new_batch, _, y_new_batch, _ = train_test_split(data.drop('is_fraud', axis=1), data['is_fraud'], test_size=0.1, random_state=42)\n",
    "    X_new_batch_imputed = imputer.transform(X_new_batch)\n",
    "    model.partial_fit(X_new_batch_imputed, y_new_batch)\n",
    "\n",
    "    # Guardar el modelo después del entrenamiento incremental\n",
    "    incremental_model_filename = f\"{model_name}_incremental_model.pkl\"\n",
    "    joblib.dump(model, incremental_model_filename)\n",
    "    print(f\"Modelo {model_name} guardado exitosamente.\")\n",
    "    \n",
    "    # Evaluación después del entrenamiento incremental\n",
    "    roc_auc_after, precision_after, recall_after, f1_after = evaluate_model(model, X_dev_imputed, y_dev)  # Utilizar X_dev_imputed\n",
    "    metrics_after_incremental[model_name] = (roc_auc_after, precision_after, recall_after, f1_after)\n",
    "\n",
    "    print(f\"Modelo {model_name} entrenado y evaluado exitosamente.\")\n",
    "    print()\n",
    "    print(\"Antes del entrenamiento incremental:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_before}, Precisión: {precision_before}, Recall: {recall_before}, F1-score: {f1_before}\")\n",
    "    print(\"Después del entrenamiento incremental:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_after}, Precisión: {precision_after}, Recall: {recall_after}, F1-score: {f1_after}\")\n",
    "\n",
    "# Imprimir las métricas antes y después del entrenamiento incremental para cada modelo\n",
    "for model_name, (roc_auc_before, precision_before, recall_before, f1_before), (roc_auc_after, precision_after, recall_after, f1_after) in zip(metrics_before_incremental.keys(), metrics_before_incremental.values(), metrics_after_incremental.values()):\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    print(\"Antes del entrenamiento incremental:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_before}, Precisión: {precision_before}, Recall: {recall_before}, F1-score: {f1_before}\")\n",
    "    print(\"Después del entrenamiento incremental:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_after}, Precisión: {precision_after}, Recall: {recall_after}, F1-score: {f1_after}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.999940938211968, Precisión: 0.9970716435232042, Recall: 1.0, F1-score: 0.9985336748001541\n",
      "Después del reentrenamiento incremental:\n",
      "ROC-AUC: 0.9999737131241117, Precisión: 0.9978930877069772, Recall: 1.0, F1-score: 0.9989454329133092\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    # Predecir probabilidades\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "    precision = precision_score(y, model.predict(X))\n",
    "    recall = recall_score(y, model.predict(X))\n",
    "    f1 = f1_score(y, model.predict(X))\n",
    "    \n",
    "    return roc_auc, precision, recall, f1\n",
    "\n",
    "# Cargar el modelo\n",
    "model = joblib.load('XGBoost_initial_model.pkl')\n",
    "\n",
    "roc_auc_before, precision_before, recall_before, f1_before = evaluate_model(model, X_dev_imputed, y_dev)  # Utilizar X_dev_imputed\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc_before}, Precisión: {precision_before}, Recall: {recall_before}, F1-score: {f1_before}\")\n",
    "\n",
    "# Entrenamiento incremental con nuevos datos nunca antes utilizados por el modelo\n",
    "X_new_batch, _, y_new_batch, _ = train_test_split(data.drop('is_fraud', axis=1), data['is_fraud'], test_size=0.1, random_state=42)\n",
    "X_new_batch_imputed = imputer.transform(X_new_batch)\n",
    "\n",
    "# Combinar datos originales con nuevos datos\n",
    "X_combined = np.concatenate((X_dev_imputed, X_new_batch_imputed), axis=0)\n",
    "y_combined = np.concatenate((y_dev, y_new_batch), axis=0)\n",
    "\n",
    "# Reentrenar el modelo con los datos combinados\n",
    "model.fit(X_combined, y_combined)\n",
    "\n",
    "# Evaluar el modelo actualizado\n",
    "roc_auc_after, precision_after, recall_after, f1_after = evaluate_model(model, X_dev_imputed, y_dev)\n",
    "print(f\"Después del reentrenamiento incremental:\")\n",
    "print(f\"ROC-AUC: {roc_auc_after}, Precisión: {precision_after}, Recall: {recall_after}, F1-score: {f1_after}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5002959966659546\n",
      "Epoch 10, Loss: 0.5002959966659546\n",
      "Epoch 20, Loss: 0.5002959966659546\n",
      "Epoch 30, Loss: 0.5002959966659546\n",
      "Epoch 40, Loss: 0.5002959966659546\n",
      "Epoch 50, Loss: 0.5002959966659546\n",
      "Epoch 60, Loss: 0.5002959966659546\n",
      "Epoch 70, Loss: 0.5002959966659546\n",
      "Epoch 80, Loss: 0.5002959966659546\n",
      "Epoch 90, Loss: 0.5002959966659546\n",
      "9214/9214 [==============================] - 6s 635us/step\n",
      "ROC-AUC: 0.49299079804473644, Precisión: 0.5011955677505351, Recall: 1.0, F1-score: 0.6677285471892926\n",
      "112868/112868 [==============================] - 75s 662us/step - loss: inf\n",
      "9214/9214 [==============================] - 5s 535us/step\n",
      "Después del reentrenamiento incremental:\n",
      "ROC-AUC: 0.5069611277327026, Precisión: 0.0, Recall: 0.0, F1-score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SVM_incremental_model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def evaluate_model(model, X, y, threshold=0.5):\n",
    "    # Predecir probabilidades\n",
    "    y_pred_proba = model.predict(X)\n",
    "\n",
    "    # Convertir probabilidades a etiquetas binarias\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calcular métricas\n",
    "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "\n",
    "    return roc_auc, precision, recall, f1\n",
    "\n",
    "# Definir hiperparámetros\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Definir el modelo SVM lineal en TensorFlow\n",
    "class LinearSVM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LinearSVM, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(1, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "# Convertir los datos a tensores de TensorFlow\n",
    "X_train_tensor = tf.constant(X_train_imputed, dtype=tf.float32)\n",
    "y_train_tensor = tf.constant(y_train.values.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "# Crear el modelo SVM lineal\n",
    "model = LinearSVM()\n",
    "\n",
    "# Definir la función de pérdida (hinge loss)\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.maximum(0., 1. - y_true * y_pred))\n",
    "\n",
    "# Definir el optimizador\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = hinge_loss(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step(X_train_tensor, y_train_tensor)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.numpy()}')\n",
    "\n",
    "\n",
    "roc_auc_before, precision_before, recall_before, f1_before = evaluate_model(model, X_dev_imputed, y_dev)  # Utilizar X_dev_imputed\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc_before}, Precisión: {precision_before}, Recall: {recall_before}, F1-score: {f1_before}\")\n",
    "\n",
    "initial_model_filename = \"SVM_initial_model.pkl\"\n",
    "joblib.dump(model, initial_model_filename)\n",
    "\n",
    "# Entrenamiento incremental con nuevos datos nunca antes utilizados por el modelo\n",
    "X_new_batch, _, y_new_batch, _ = train_test_split(data.drop('is_fraud', axis=1), data['is_fraud'], test_size=0.1, random_state=42)\n",
    "X_new_batch_imputed = imputer.transform(X_new_batch)\n",
    "\n",
    "# Combinar datos originales con nuevos datos\n",
    "X_combined = np.concatenate((X_dev_imputed, X_new_batch_imputed), axis=0)\n",
    "y_combined = np.concatenate((y_dev, y_new_batch), axis=0)\n",
    "\n",
    "# Compilar el modelo con un optimizador y una función de pérdida\n",
    "model.compile(optimizer='sgd', loss='hinge')\n",
    "\n",
    "# Reentrenar el modelo con los datos combinados\n",
    "model.fit(X_combined, y_combined)\n",
    "\n",
    "# Evaluar el modelo actualizado\n",
    "roc_auc_after, precision_after, recall_after, f1_after = evaluate_model(model, X_dev_imputed, y_dev)\n",
    "print(\"Después del reentrenamiento incremental:\")\n",
    "print(f\"ROC-AUC: {roc_auc_after}, Precisión: {precision_after}, Recall: {recall_after}, F1-score: {f1_after}\")\n",
    "\n",
    "initial_model_filename = \"SVM_incremental_model.pkl\"\n",
    "joblib.dump(model, initial_model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
