{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2\n",
    "\n",
    "## Diego Franco - 20240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Lee el archivo CSV\n",
    "data = pd.read_csv('BalanceData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Eliminar la columna 'trans_date_trans_time'\n",
    "data.drop(columns=['trans_date_trans_time', 'transaction_time'], inplace=True)\n",
    "\n",
    "# Codificar variables categóricas con label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job']\n",
    "for col in categorical_cols:\n",
    "    data[col + '_encoded'] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Eliminar columnas originales no numéricas y otras columnas irrelevantes\n",
    "data.drop(columns=['merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job', 'dob', 'trans_num'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento, conjunto de validación (dev) y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('is_fraud', axis=1), data['is_fraud'], test_size=0.2, random_state=42)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Imputar valores faltantes con la estrategia de relleno con el valor medio\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_dev_imputed = imputer.transform(X_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo SVM...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer \n",
    "import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    # Predecir probabilidades\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "    precision = precision_score(y, model.predict(X))\n",
    "    recall = recall_score(y, model.predict(X))\n",
    "    f1 = f1_score(y, model.predict(X))\n",
    "    \n",
    "    return roc_auc, precision, recall, f1\n",
    "\n",
    "# Inicializar los modelos\n",
    "ann_model = MLPClassifier()\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "rf_model = RandomForestClassifier()\n",
    "svm_model = SVC(probability=True)  # Habilitar la predicción de probabilidades para SVM\n",
    "\n",
    "# Diccionario de modelos\n",
    "models = {\n",
    "    #\"LightGBM\": lgb_model,\n",
    "    #\"XGBoost\": xgb_model,\n",
    "    #\"Random Forest\": rf_model,\n",
    "    \"SVM\": svm_model\n",
    "}\n",
    "\n",
    "# Imputar valores faltantes con la estrategia de relleno con el valor medio\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_dev_imputed = imputer.transform(X_dev)\n",
    "\n",
    "# Diccionario para almacenar métricas antes y después del entrenamiento incremental\n",
    "metrics_before_incremental = {}\n",
    "metrics_after_incremental = {}\n",
    "\n",
    "# Iterar sobre cada modelo\n",
    "for model_name, model in models.items():\n",
    "\n",
    "    print(f\"Entrenando modelo {model_name}...\")\n",
    "    # Entrenamiento inicial\n",
    "    model.fit(X_train_imputed, y_train)  # Utilizar X_train_imputed\n",
    "\n",
    "    # Guardar el modelo antes del entrenamiento incremental\n",
    "    initial_model_filename = f\"{model_name}_initial_model.pkl\"\n",
    "    joblib.dump(model, initial_model_filename)\n",
    "    print(f\"Modelo {model_name} guardado exitosamente.\")\n",
    "    \n",
    "    # Evaluación inicial\n",
    "    roc_auc_before, precision_before, recall_before, f1_before = evaluate_model(model, X_dev_imputed, y_dev)  # Utilizar X_dev_imputed\n",
    "    metrics_before_incremental[model_name] = (roc_auc_before, precision_before, recall_before, f1_before)\n",
    "    \n",
    "    # Entrenamiento incremental con nuevos datos nunca antes utilizados por el modelo\n",
    "    X_new_batch, _, y_new_batch, _ = train_test_split(data.drop('is_fraud', axis=1), data['is_fraud'], test_size=0.1, random_state=42)\n",
    "    X_new_batch_imputed = imputer.transform(X_new_batch)\n",
    "    model.partial_fit(X_new_batch_imputed, y_new_batch)\n",
    "\n",
    "    # Guardar el modelo después del entrenamiento incremental\n",
    "    incremental_model_filename = f\"{model_name}_incremental_model.pkl\"\n",
    "    joblib.dump(model, incremental_model_filename)\n",
    "    print(f\"Modelo {model_name} guardado exitosamente.\")\n",
    "    \n",
    "    # Evaluación después del entrenamiento incremental\n",
    "    roc_auc_after, precision_after, recall_after, f1_after = evaluate_model(model, X_dev_imputed, y_dev)  # Utilizar X_dev_imputed\n",
    "    metrics_after_incremental[model_name] = (roc_auc_after, precision_after, recall_after, f1_after)\n",
    "\n",
    "    print(f\"Modelo {model_name} entrenado y evaluado exitosamente.\")\n",
    "    print()\n",
    "    print(\"Antes del entrenamiento incremental:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_before}, Precisión: {precision_before}, Recall: {recall_before}, F1-score: {f1_before}\")\n",
    "    print(\"Después del entrenamiento incremental:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_after}, Precisión: {precision_after}, Recall: {recall_after}, F1-score: {f1_after}\")\n",
    "\n",
    "# Imprimir las métricas antes y después del entrenamiento incremental para cada modelo\n",
    "for model_name, (roc_auc_before, precision_before, recall_before, f1_before), (roc_auc_after, precision_after, recall_after, f1_after) in zip(metrics_before_incremental.keys(), metrics_before_incremental.values(), metrics_after_incremental.values()):\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    print(\"Antes del entrenamiento incremental:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_before}, Precisión: {precision_before}, Recall: {recall_before}, F1-score: {f1_before}\")\n",
    "    print(\"Después del entrenamiento incremental:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_after}, Precisión: {precision_after}, Recall: {recall_after}, F1-score: {f1_after}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los resultados antes del entrenamiento incremental a un DataFrame\n",
    "results_before_df = pd.DataFrame({\n",
    "    'Modelo': list(metrics_before_incremental.keys()),\n",
    "    'ROC-AUC': [metric[0] for metric in metrics_before_incremental.values()],\n",
    "    'Precisión': [metric[1] for metric in metrics_before_incremental.values()],\n",
    "    'Recall': [metric[2] for metric in metrics_before_incremental.values()],\n",
    "    'F1-score': [metric[3] for metric in metrics_before_incremental.values()]\n",
    "})\n",
    "\n",
    "# Guardar el DataFrame de resultados antes del entrenamiento incremental en un archivo CSV\n",
    "results_before_df.to_csv('resultados_before_entrenamiento_incremental.csv', index=False)\n",
    "\n",
    "# Convertir los resultados después del entrenamiento incremental a un DataFrame\n",
    "results_after_df = pd.DataFrame({\n",
    "    'Modelo': list(metrics_after_incremental.keys()),\n",
    "    'ROC-AUC': [metric[0] for metric in metrics_after_incremental.values()],\n",
    "    'Precisión': [metric[1] for metric in metrics_after_incremental.values()],\n",
    "    'Recall': [metric[2] for metric in metrics_after_incremental.values()],\n",
    "    'F1-score': [metric[3] for metric in metrics_after_incremental.values()]\n",
    "})\n",
    "\n",
    "# Guardar el DataFrame de resultados después del entrenamiento incremental en un archivo CSV\n",
    "results_after_df.to_csv('resultados_after_entrenamiento_incremental.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
